{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bz7dXD8Y5e7V"
   },
   "source": [
    "<CENTER> <B> <U> Tokenize Non-English Languages </CENTER> </B> </U>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soYVsByjOCNI"
   },
   "source": [
    "## <b>ToktokTokenizer</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3bDbhw3DfRU"
   },
   "source": [
    "The tok-tok tokenizer is a simple, general tokenizer, where the input has one sentence per line; thus only final period is tokenized. Tok-tok has been tested on, and gives reasonably good results for English, Persian, Russian, Czech, French, German, Vietnamese, Tajik, and a few others. The input should be in UTF-8 encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hTGwsnzk5YcD"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 355,
     "status": "ok",
     "timestamp": 1629979273417,
     "user": {
      "displayName": "R AKHILANDESHWARI 2048046",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIkCurCgc78Nn4rfHenbXpNzRwE23THtku9QZFlQ=s64",
      "userId": "00252764270953512856"
     },
     "user_tz": -330
    },
    "id": "PrD5t-WzEDto"
   },
   "outputs": [],
   "source": [
    "from nltk import ToktokTokenizer\n",
    "toktok = ToktokTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzLPXpcBFxQ_"
   },
   "source": [
    "We will tokenize the statemnt --> \"The world is healing now\" using different languages like\n",
    "1. Persian\n",
    "2. Russian\n",
    "3. French\n",
    "4. German\n",
    "5. Czech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 366,
     "status": "ok",
     "timestamp": 1629979287722,
     "user": {
      "displayName": "R AKHILANDESHWARI 2048046",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIkCurCgc78Nn4rfHenbXpNzRwE23THtku9QZFlQ=s64",
      "userId": "00252764270953512856"
     },
     "user_tz": -330
    },
    "id": "KySySpUiGFCU",
    "outputId": "b63387b6-d030-481c-ece1-da66f41d809c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " English : -  ['The', 'world', 'is', 'healing', 'now']\n",
      " Persian : -  ['دنيا', 'الان', 'داره', 'خوب', 'ميکنه']\n",
      " Russian : -  ['Мир', 'исцеляется', 'сейчас']\n",
      " French : -  ['Le', 'monde', 'guérit', 'maintenant']\n",
      " German : -  ['Die', 'Welt', 'heilt', 'jetzt']\n",
      " Czech : -  ['Svět', 'se', 'nyní', 'uzdravuje']\n"
     ]
    }
   ],
   "source": [
    "# English \n",
    "text = 'The world is healing now'\n",
    "print(\" English : - \", toktok.tokenize(text))\n",
    "\n",
    "# Persian \n",
    "text = u'دنيا الان داره خوب ميکنه'\n",
    "print(\" Persian : - \", toktok.tokenize(text))\n",
    "\n",
    "# Russian\n",
    "text = 'Мир исцеляется сейчас'\n",
    "print(\" Russian : - \", toktok.tokenize(text))\n",
    "\n",
    "# French\n",
    "text = 'Le monde guérit maintenant'\n",
    "print(\" French : - \", toktok.tokenize(text))\n",
    "\n",
    "# German\n",
    "text = 'Die Welt heilt jetzt'\n",
    "print(\" German : - \", toktok.tokenize(text))\n",
    "\n",
    "# Czech\n",
    "text = 'Svět se nyní uzdravuje'\n",
    "print(\" Czech : - \", toktok.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJELkDBSIovk"
   },
   "source": [
    "## <b> spacy </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1394,
     "status": "ok",
     "timestamp": 1629979294899,
     "user": {
      "displayName": "R AKHILANDESHWARI 2048046",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIkCurCgc78Nn4rfHenbXpNzRwE23THtku9QZFlQ=s64",
      "userId": "00252764270953512856"
     },
     "user_tz": -330
    },
    "id": "6NHS6-yXIsvk"
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.lang.de import German\n",
    "\n",
    "nlp_en = English()  # Includes English data\n",
    "nlp_de = German()  # Includes German data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 353,
     "status": "ok",
     "timestamp": 1629979304131,
     "user": {
      "displayName": "R AKHILANDESHWARI 2048046",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIkCurCgc78Nn4rfHenbXpNzRwE23THtku9QZFlQ=s64",
      "userId": "00252764270953512856"
     },
     "user_tz": -330
    },
    "id": "ypFq8XBVJcz-",
    "outputId": "5cfe184c-8ecb-4fc7-a9fd-c4817f721056"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token.text \t token.tag_ \t token.shape_ \t token.is_alpha \t token.is_stop\n",
      "Apple t\t  \t\t Xxxxx \t\t True \t\t False\n",
      "erschürf t\t  \t\t xxxx \t\t True \t\t False\n",
      "sich t\t  \t\t xxxx \t\t True \t\t True\n",
      "den t\t  \t\t xxx \t\t True \t\t True\n",
      "Kauf t\t  \t\t Xxxx \t\t True \t\t False\n",
      "eines t\t  \t\t xxxx \t\t True \t\t True\n",
      "britischen t\t  \t\t xxxx \t\t True \t\t False\n",
      "Startups t\t  \t\t Xxxxx \t\t True \t\t False\n",
      "für t\t  \t\t xxx \t\t True \t\t True\n",
      "1 t\t  \t\t d \t\t False \t\t False\n",
      "Milliarde t\t  \t\t Xxxxx \t\t True \t\t False\n",
      "US-Dollar t\t  \t\t XX-Xxxxx \t\t False \t\t False\n"
     ]
    }
   ],
   "source": [
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp_de(\"Apple erschürf sich den Kauf eines britischen Startups für 1 Milliarde US-Dollar\")\n",
    "print ('token.text \\t token.tag_ \\t token.shape_ \\t token.is_alpha \\t token.is_stop')\n",
    "for token in doc:\n",
    "    print(token.text,'t\\t', token.tag_,'\\t\\t',token.shape_,'\\t\\t', token.is_alpha,'\\t\\t', token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvdcx3odZrcl"
   },
   "source": [
    "# <b> word_tokenize </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIUx5yTKW67t"
   },
   "source": [
    "## Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 365,
     "status": "ok",
     "timestamp": 1629979672692,
     "user": {
      "displayName": "R AKHILANDESHWARI 2048046",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIkCurCgc78Nn4rfHenbXpNzRwE23THtku9QZFlQ=s64",
      "userId": "00252764270953512856"
     },
     "user_tz": -330
    },
    "id": "KAftrp32W-6t"
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 621,
     "status": "ok",
     "timestamp": 1629979689150,
     "user": {
      "displayName": "R AKHILANDESHWARI 2048046",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIkCurCgc78Nn4rfHenbXpNzRwE23THtku9QZFlQ=s64",
      "userId": "00252764270953512856"
     },
     "user_tz": -330
    },
    "id": "P51dEGtpXAkr"
   },
   "outputs": [],
   "source": [
    "tokenize_spanish = nltk.data.load('tokenizers/punkt/PY3/spanish.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1629979706113,
     "user": {
      "displayName": "R AKHILANDESHWARI 2048046",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIkCurCgc78Nn4rfHenbXpNzRwE23THtku9QZFlQ=s64",
      "userId": "00252764270953512856"
     },
     "user_tz": -330
    },
    "id": "1r3mT29mXEmc"
   },
   "outputs": [],
   "source": [
    "# English - Hi all, your learning tokenization of different languages.\n",
    "Sample_text = \"Hola a todos, su aprendizaje de tokenización de diferentes idiomas.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 356,
     "status": "ok",
     "timestamp": 1629980190960,
     "user": {
      "displayName": "R AKHILANDESHWARI 2048046",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIkCurCgc78Nn4rfHenbXpNzRwE23THtku9QZFlQ=s64",
      "userId": "00252764270953512856"
     },
     "user_tz": -330
    },
    "id": "YaB1HRWVXLAC",
    "outputId": "c989838d-517c-416c-c1ec-1fe82096ce83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hola a todos, su aprendizaje de tokenización de diferentes idiomas.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_spanish.tokenize(Sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 343,
     "status": "ok",
     "timestamp": 1629980206545,
     "user": {
      "displayName": "R AKHILANDESHWARI 2048046",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIkCurCgc78Nn4rfHenbXpNzRwE23THtku9QZFlQ=s64",
      "userId": "00252764270953512856"
     },
     "user_tz": -330
    },
    "id": "GvNs9432Yv_6",
    "outputId": "95f8293f-835f-4583-e1c9-8425efb8d962"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hola',\n",
       " 'a',\n",
       " 'todos',\n",
       " ',',\n",
       " 'su',\n",
       " 'aprendizaje',\n",
       " 'de',\n",
       " 'tokenización',\n",
       " 'de',\n",
       " 'diferentes',\n",
       " 'idiomas',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(Sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iD87C_oqZEIN"
   },
   "source": [
    "## French Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 354,
     "status": "ok",
     "timestamp": 1629980231244,
     "user": {
      "displayName": "R AKHILANDESHWARI 2048046",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIkCurCgc78Nn4rfHenbXpNzRwE23THtku9QZFlQ=s64",
      "userId": "00252764270953512856"
     },
     "user_tz": -330
    },
    "id": "pfdqcEUVYqy9",
    "outputId": "a7b7dfc9-745c-4736-fac7-bf56f7853f22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John',\n",
       " 'Richard',\n",
       " 'Bond',\n",
       " 'explique',\n",
       " 'le',\n",
       " 'rôle',\n",
       " 'de',\n",
       " \"l'astronomie\",\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_french = (\"John Richard Bond explique le rôle de l'astronomie.\")\n",
    "word_tokenize(content_french, language='french')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfUSOGiFMmCU"
   },
   "source": [
    "#  <b>Inference  </b>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVzWeQAGOHoh"
   },
   "source": [
    "I have worked with 2 different modules. \n",
    "1. TokTokToenizer\n",
    "2. Spacy\n",
    "3. word_tokenizer\n",
    "\n",
    "The TokTokTokenizer does the basic level tokenization for only few set of languages. \n",
    "\n",
    "Spacy can support 60+ different languagues for tokenization. Through Spacy, we can even get the named entity recognisation in just one line of word, we can ffind the size, position, is_stop, lemmas, tags of a tokenized sentance. It also tells whether a given word starts with capital letter or lower case letter. \n",
    "\n",
    "While directly tokenizing different languages, the tokenization is not done properly. Whereas, if we use the word_tokenize and give the \"lang\" parameter then we could tokenize efficiently. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNS9GD6p+EFAYqC/iD7r+mj",
   "name": "PROGRAM3_2048046.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
